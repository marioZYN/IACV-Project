{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.feature\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Extraction Example  \n",
    "I use blob detection to extract the dotted-area in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data initialization\n",
    "file_names = ['0.jpg']\n",
    "classes = [\"adult_males\", \"subadult_males\", \"adult_females\", \"juveniles\", \"pups\"]\n",
    "coordinates_df = pd.DataFrame(index=file_names, columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:11111: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05ce1099de9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# mask out blackened regions from Train Dotted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmask_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmask_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmask_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:11111: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "# get coordinates\n",
    "for filename in file_names:\n",
    "\n",
    "    # read the Train and Train Dotted images\n",
    "    image_1 = cv2.imread(\"./data/train-dotted/\" + filename)\n",
    "    image_2 = cv2.imread(\"./data/train/\" + filename)\n",
    "\n",
    "    # absolute difference between Train and Train Dotted\n",
    "    image_3 = cv2.absdiff(image_1,image_2)\n",
    "\n",
    "    # mask out blackened regions from Train Dotted\n",
    "    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n",
    "    mask_1[mask_1 < 20] = 0\n",
    "    mask_1[mask_1 > 0] = 255\n",
    "\n",
    "    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
    "    mask_2[mask_2 < 20] = 0\n",
    "    mask_2[mask_2 > 0] = 255\n",
    "\n",
    "    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n",
    "    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_2) \n",
    "\n",
    "    # convert to grayscale to be accepted by skimage.feature.blob_log\n",
    "    image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect blobs\n",
    "    blobs = skimage.feature.blob_log(image_3, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n",
    "\n",
    "    adult_males = []\n",
    "    subadult_males = []\n",
    "    pups = []\n",
    "    juveniles = []\n",
    "    adult_females = [] \n",
    "\n",
    "    for blob in blobs:\n",
    "        # get the coordinates for each blob\n",
    "        y, x, s = blob\n",
    "        # get the color of the pixel from Train Dotted in the center of the blob\n",
    "        g,b,r = image_1[int(y)][int(x)][:]\n",
    "\n",
    "        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n",
    "        if r > 200 and g < 50 and b < 50: # RED\n",
    "            adult_males.append((int(x),int(y)))        \n",
    "        elif r > 200 and g > 200 and b < 50: # MAGENTA\n",
    "            subadult_males.append((int(x),int(y)))         \n",
    "        elif r < 100 and g < 100 and 150 < b < 200: # GREEN\n",
    "            pups.append((int(x),int(y)))\n",
    "        elif r < 100 and  100 < g and b < 100: # BLUE\n",
    "            juveniles.append((int(x),int(y))) \n",
    "        elif r < 150 and g < 50 and b < 100:  # BROWN\n",
    "            adult_females.append((int(x),int(y)))\n",
    "\n",
    "    coordinates_df[\"adult_males\"][filename] = adult_males\n",
    "    coordinates_df[\"subadult_males\"][filename] = subadult_males\n",
    "    coordinates_df[\"adult_females\"][filename] = adult_females\n",
    "    coordinates_df[\"juveniles\"][filename] = juveniles\n",
    "    coordinates_df[\"pups\"][filename] = pups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 50 x 50 patches\n",
    "x = []\n",
    "y = []\n",
    "for filename in file_names:    \n",
    "    image = cv2.imread(\"../Data/Train/\" + filename)\n",
    "    for lion_class in classes:\n",
    "        for coordinates in coordinates_df[lion_class][filename]:\n",
    "            thumb = image[coordinates[1]-25:coordinates[1]+25,coordinates[0]-25:coordinates[0]+25,:]\n",
    "            if np.shape(thumb) == (50, 50, 3):\n",
    "                x.append(thumb)\n",
    "                y.append(lion_class)\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples\n",
    "for lion_class in classes:\n",
    "    f, ax = plt.subplots(1,10,figsize=(12,1.5))\n",
    "    f.suptitle(lion_class)\n",
    "    axes = ax.flatten()\n",
    "    j = 0\n",
    "    for a in axes:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "        for i in range(j,len(x)):\n",
    "            if y[i] == lion_class:\n",
    "                j = i+1\n",
    "                a.imshow(cv2.cvtColor(x[i], cv2.COLOR_BGR2RGB))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use Keras to train from a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, Cropping2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
